\chapter{总结与展望}
\label{cha:conclusions}
本章首先对本文的研究内容及其与已有工作的异同进行了总结，概括了本文工作的贡献和不足。随后，对多任务自然语言处理的研究前景进行了展望。

\section{工作总结}
本文研究了四种多任务Transformer架构，其中包含两种传统的硬共享结构，也包含两种新颖的逐层共享结构。据我们所知，本文是首次较为系统地在Transformer模型上研究多任务共享架构。近期，Liu等人\cite{liu2019multi}在BERT\cite{devlin2018bert}的基础上提出了一种多任务Transformer模型MT-DNN，在多个NLP任务上相较原始的单任务学习模型提升了性能。然而，MT-DNN的模型架构仍是简单的硬共享模式，并没有为Transformer设计新的共享结构。本文的工作除了验证硬共享模式在Transformer上的有效性之外，还针对Transformer的结构特点设计了两种简单高效的逐层共享模式：L-I结构和L-E结构。

逐层共享结构使得模型能够在每一层都形成自己的任务特定表示，而不局限于神经网络的顶层。事实上，多任务学习中另一种被广泛使用的共享模式 —— 软共享 —— 也可以在每一层形成任务特定表示。本文的L-I结构和L-E结构与传统的软共享模式的区别在于，L-I结构和L-E结构基于注意力机制，能够动态地根据输入来决定与哪个任务共享，以及共享特征的程度。简单地说，L-I结构和L-E结构更加灵活，能够自行决定何时共享、与谁共享、共享多少。另外，随着共享任务数量的增多，L-I结构和L-E结构只需要增加少量的参数，而软共享往往需要增加大量参数，难以扩展。

本文还在16个文本分类数据集上进行了实验，结果表明多任务Transformer模型一致性地超越了单任务学习模型的性能，并且L-I结构和L-E结构相对传统的硬共享结构取得了更好的效果。最后，讨论了四种结构对于网络层数的敏感性，并通过可视化实例分析解释了模型的工作机理，揭示了任务之间的相关性。

然而，本文提出的多任务学习模型架构也具有一定的局限性，只能适用于句子级别的任务，如文本分类、自然语言推理，难以应用于序列标注任务中。如何为序列标注任务设计有效的多任务Transformer结构仍是需要解决的问题。

\section{未来展望}
考察进入深度学习时代以来自然语言处理领域的研究进展，容易发现很多模型性能的重大突破都来自于预训练方法的改进，如word2vec\cite{DBLP:conf/nips/MikolovSCCD13}、ELMo\cite{DBLP:conf/naacl/PetersNIGCLZ18}和BERT\cite{devlin2018bert}，这些预训练模型往往是通用的、可迁移的，因而在很多任务中都能带来明显的性能提升。随着这些通用模型的发展，越来越多的人开始思考一个令人兴奋的命题：是否能够训练单一模型来处理几乎所有的NLP任务？

然而，目前最强大的BERT也还无法成为这样的“单一模型”。近期的相关实验表明，BERT在语义任务以及需要任务特定语法知识的任务上表现欠佳，对实体和指代现象的处理还不够好\cite{tenney2018you}\cite{liu2019linguistic}，这说明，BERT在某些情况下也并非如此“通用”。如何赋予预训练模型更多知识呢？

近期，百度对BERT的训练任务进行了改进，提出了基于中文的预训练模型ERNIE，它具备了更多的实体知识。但其并未在根本上解决BERT面临的局限性。

造成这一局限性的一个关键原因在于，用来训练BERT的任务 —— 完形填空和预测下一句话 —— 并不能对所有NLP任务都带来很大提升。事实上，可能不存在某个单一预训练任务在所有目标任务下都非常有效。在这一背景下，一个很自然的想法便是利用多任务学习方法来训练。

本质上，迁移学习和多任务学习都是通过参数共享的方式来起作用。BERT采用的模型结构便是Transformer，因而BERT的成功证明了Transformer强大的知识存储能力和可迁移性，这为多任务Transformer的有效性提供了保障。我相信，随着对多任务Transformer结构的探索，越来越多的任务可以被同时训练，也会有越来越多的任务知识可以共存于单一模型中，对基于多任务学习的文本表示方法的研究将会带来更加强大的通用模型。

