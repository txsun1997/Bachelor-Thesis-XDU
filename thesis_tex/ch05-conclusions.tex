\chapter{总结与展望}
\label{cha:conclusions}
本章首先对已有工作以及本文的研究内容进行了总结，概括了本文工作的贡献，也指出了不足。随后，对多任务自然语言处理的研究前景进行了展望，给出了未来的的研究方向。

\section{工作总结}
学习文本的分布式表示一直以来都是基于神经网络的自然语言处理模型的关键问题，文本数据特征表示的好坏直接决定着模型的性能。在数据量或计算资源有限的情况下，传统的单任务方法常常难以学习到泛化的特征表示。因此，多任务学习作为一种学习泛化表示的手段被广泛应用到各类神经网络中。

多任务学习的本质在于参数共享，如何设计有效的共享模式一直是多任务学习的研究重点。一方面，多任务学习中的很多共享模式是模型无关的，可以不加修改地应用在各类模型上，如硬共享模式。另一方面，对于特定的网络和任务常常又需要设计特定的共享模式。

近年来，很多研究人员在卷积神经网络和循环神经网络上应用多任务学习，在自然语言处理、计算机视觉、语音识别等应用领域都取得了成功。然而，目前还很少有工作探索多任务学习在Transformer上的应用模式和效果。本文旨在一定程度上填补这一空缺，验证了多任务学习在Transformer结构上的有效性，并针对Transformer结构特点提出了两种新型的共享模式：L-I共享模式和L-E共享模式，在多个文本分类任务上相比传统的硬共享模式取得了更高的准确率。本文的实验结果表明，在神经网络的每一层都提取任务特定表示具有

然而，本文提出的多任务学习模型架构也具有一定的局限性，只能适用于句子级别的任务，如文本分类任务，难以应用于序列标注任务中。如何为序列标注任务设计有效的多任务Transformer结构仍是需要解决的问题。

\section{未来展望}
目前主流的单任务学习方法常常使得神经网络模型受限于单一任务甚至数据集，在某个数据集上表现良好的模型可能在另一个数据集上表现很差。通过设置合适的训练任务，增大训练数据量可以得到具备强大泛化表示能力的可迁移模型，能够为很多NLP任务带来性能提升，这类方法的典型代表就是BERT。由于其带来的强大的性能提升和通用性，BERT在NLP社区获得了巨大反响，也引发了很多研究者的思考：是否能够训练单一模型来处理几乎所有的NLP任务？

但是近期的相关实验也表明，BERT在语义任务以及需要任务特定语法知识的任务上表现欠佳，对实体和指代现象的处理还不够好\cite{47786}\cite{liu2019linguistic}，这说明，BERT在某些情况下也并非如此“通用”。如何赋予预训练模型更多知识呢？

近期，百度对BERT的训练任务进行了改进，提出了基于中文的预训练模型ERNIE，它具备了更多的实体知识。但其并未在根本上解决BERT面临的局限性。

造成这一局限性的一个关键原因在于，用来训练BERT的任务——语言模型和预测下一句话——并不能对所有NLP任务都带来很大提升。事实上，可能不存在某个单一预训练任务在所有目标任务下都非常有效。在这一背景下，一个很自然的想法便是利用多任务学习方法来训练。

事实上，迁移学习和多任务学习都是通过参数共享的方式来起作用。BERT采用的模型结构便是Transformer，因而BERT的成功也证明了Transformer强大的知识存储能力和可迁移性，这为多任务Transformer的有效性提供了保障。在BERT基础上，进一步使用多任务训练的方法已经被证明是有效的\cite{liu2019multi}。我相信，随着对多任务Transformer结构的探索，越来越多的任务可以被同时训练，相应的，也会有越来越多的任务知识可以共存于单一模型中。

