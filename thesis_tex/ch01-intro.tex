\chapter{绪论}
\label{cha:intro}
本章首先介绍基于深度学习的自然语言处理的研究背景，阐述在该背景下多任务学习的研究价值及意义。接着简要介绍自然语言处理和多任务学习的研究进展，并指出目前存在的问题。然后介绍本文的研究内容及目标，并概括了本文工作的创新之处。本章的最后给出了论文的主要内容和章节安排。

\section{研究背景及意义}
1950年，阿兰·图灵（Alan Turing）提出了著名的\emph{图灵测试}\footnote{图灵测试是指，一个人在不接触对方的情况下，通过某种方式和对方进行一系列的问答。若在相当长时间内，他无法根据问答的情况判断对方是人还是机器，那么可以认为该机器具备智能。}，直接推动了人工智能从哲学探讨的层面上升到科学研究。随后不久，在1956年举办的达特茅斯会议上，\emph{人工智能}的概念被正式提出，John McCarthy将这一新兴领域的研究目标定义为：“让机器的行为看起来像人类所表现出来的智能行为一样”。自1956年至今的六十余年中，研究人员尝试了多种方法来实现这一愿景，人工智能领域也随着这些方法的成功与失败经历了数次热潮与低谷。近年来，随着数据量的增加和算力的增强，以神经网络为代表的\emph{深度学习}异军突起，在语音识别\cite{DBLP:conf/asru/MikolovDPBC11}\cite{DBLP:conf/apsipa/LiHYW13}、计算机视觉\cite{DBLP:journals/cacm/KrizhevskySH17}\cite{DBLP:journals/pami/FarabetCNL13}\cite{DBLP:conf/nips/TompsonJLB14}等众多应用场景中取得了巨大突破，也为自然语言处理领域带来了深刻的变革。

自然语言，即文明发展过程中自然形成的语言，是最能体现人类智慧和文明的产物。\emph{自然语言处理}（Natural Language Processing, NLP）被很多人认为是“人工智能皇冠上的明珠”，致力于使用计算机技术处理、理解和生成人类语言。随着深度学习技术的发展，越来越多的研究者开始使用深度学习的技术解决自然语言处理中的难题，在很多任务上远远超越了之前的传统方法\cite{DBLP:journals/jmlr/CollobertWBKKK11}\cite{DBLP:conf/emnlp/BordesCW14}\cite{DBLP:conf/acl/JeanCMB15}\cite{DBLP:conf/nips/SutskeverVL14}。

然而，对NLP问题的研究常常被划分为多个任务，如命名实体识别、阅读理解、机器翻译等。目前一般的做法是为当前关注的某一个任务设计特定的神经网络模型，在此单一任务及数据集上进行训练。遗憾的是，这样设计出来的模型常常具有较大的局限性，在某个数据集上表现优秀的模型可能在另一个数据集上就会表现很差，即使这两个数据集来自同一任务。并且，由于NLP任务及数据集众多，一时之间各种神经网络结构层出不穷。深度学习刚刚帮助人们从“特征工程”中脱离出来，很快又陷入了所谓的网络“结构工程”。同时，这也将模型限制在了特定领域，难以发展出更为通用的智能系统。

为解决这一问题，很多研究人员转而寻求泛化能力更加强大、能够提取数据更一般性的特征表示的模型，而不是为每一个新的任务甚至数据集设计新的模型。很快，人们发现通过迁移学习和多任务学习能够得到这样的模型。事实上，迁移学习和多任务学习能够有效的原因都在于模型参数共享，或者说知识共享。其中，迁移学习的主要形式就是预训练一个较为通用的模型，再在目标任务和数据集上进行微调。在计算机视觉领域，在ImageNet\cite{DBLP:conf/cvpr/DengDSLL009}这样的大规模图像分类数据集上预训练的模型能够在很多图像分类任务上表现良好；在NLP领域，通过在大规模无标注文本上预训练一个语言模型也通常能够给各种下游任务带来很大收益\cite{DBLP:conf/naacl/PetersNIGCLZ18}\cite{radford2018improving}。这些发现表明，共享模型在其他任务上学习到的知识能够显著提升模型的性能。

在这一背景下，人们开始思考：是否可以训练单个模型来处理几乎所有的任务？要想得到这样的单一模型，几乎无可避免地需要借助多任务学习的方法。近年来，多任务学习被广泛地应用在自然语言处理领域中，在序列标注、文本分类、机器翻译等多个经典NLP任务上都取得了令人鼓舞的效果。随着多任务学习的引入，人们发现很多NLP任务可以归纳为统一的模型范式，如问答范式\cite{mccann2018natural}、分类范式\cite{radford2018improving}\cite{devlin2018bert}。同时，越来越多的研究者开始关注模型在多任务上的表现，出现了decaNLP\cite{mccann2018natural}、GLUE\cite{DBLP:conf/emnlp/WangSMHLB18}等大规模多任务评测数据集。随着算法和评测基准的逐渐成熟，多任务学习模型的开发受到越来越多的研究者关注。同时也需要注意到，多任务学习的研究历史不过二三十年，对基于深层神经网络的多任务学习的研究甚至更短。因此，如何为各神经网络模型及任务设计合适的共享结构，仍然是亟待探索的问题。

\section{相关研究进展}

本节将简要介绍深度学习背景下的自然语言处理、多任务学习、深度学习以及它们相结合的研究进展及现状，并阐述了它们之间的联系以及目前存在的不足。

自然语言处理（NLP）是一门旨在使得计算机具备处理、理解和生成自然语言（人类语言）能力的学科。近年来，以神经网络为代表的深度学习在自然语言处理\cite{DBLP:journals/jmlr/CollobertWBKKK11}\cite{DBLP:conf/emnlp/BordesCW14}\cite{DBLP:conf/acl/JeanCMB15}\cite{DBLP:conf/nips/SutskeverVL14}中取得了广泛的成功。然而，不同于语音、图像等连续实值信号，自然语言是由离散的符号构成，这使其难以直接作为神经网络的输入。为解决这一问题，人们使用低维稠密向量来表征文本的语义信息\cite{DBLP:conf/nips/MikolovSCCD13}\cite{DBLP:conf/emnlp/PenningtonSM14}，由于语义信息被分布到向量的各个维度，因此这种方法被称为分布式表示。随着分布式表示的引入，深度学习在自然语言处理领域得到了广泛的应用，卷积神经网络（CNN）、循环神经网络（RNN）相继被用于处理文本数据，近年来又提出了完全基于自注意力机制的全连接网络Transformer。这些神经网络的应用使得过去很多难以解决的NLP问题上取得了巨大进展。

事实上，文本的分布式表示的好坏对于模型的性能起着至关重要的作用。以分类任务为例，给定数据的一个好的表示，即使简单的线性分类器也能取得非常高的分类准确率\cite{tenney2018you}\cite{liu2019linguistic}。进入深度学习时代以来，自然语言处理领域中取得的许多突破都来自于对文本的通用表示方法的研究，如word2vec\cite{DBLP:conf/nips/MikolovSCCD13}，ELMo\cite{DBLP:conf/naacl/PetersNIGCLZ18}，BERT\cite{devlin2018bert}等。然而，相较于语音和图像数据，由于文本数据本身的离散性和歧义性，以及标注成本高、难度大等问题，如何得到一个好的文本表示仍然是自然语言处理领域的重大难题。

从机器学习的角度来看，一个好的表示方法除了能够在对应任务上表现良好，还应当具备良好的可迁移性与泛化能力，即能够在相似任务和新数据上获得较好的效果。在自然语言处理领域，常常使用\emph{多任务学习}（Multi-Task Learning, MTL）和\emph{迁移学习}（Transfer Learning）的方法来得到这样的文本表示\cite{devlin2018bert}\cite{DBLP:conf/icml/CollobertW08}。

对多任务学习较系统的研究可以追溯到1993年\cite{DBLP:conf/icml/Caruana93}，它是指同时使用多个任务对模型进行训练，使其学习到数据的某种泛化表示，该表示能够同时解释这多个任务。多任务学习作为一种模型无关的技术，在很多传统的机器学习模型以及神经网络上都可以应用。特别地，由于神经网络易于扩展的特性，多任务学习在神经网络上的应用更为方便和灵活。

在过去的几年里，很多研究人员探索了多任务学习在CNN和RNN上的应用模式，验证了基于神经网络的多任务学习在文本表示上的有效性。Collobert等人\cite{DBLP:conf/icml/CollobertW08}使用一个简单的卷积网络来同时学习词性标注、语块标注、命名实体识别、语义角色标注、语义相似度、语言模型等多个任务，超越了使用单任务训练的效果。随着循环神经网络在NLP上的广泛应用，研究者开始基于循环网络构造多任务学习框架，在机器翻译\cite{DBLP:conf/acl/DongWHYW15}、文本分类\cite{DBLP:conf/ijcai/LiuQH16}\cite{DBLP:conf/acl/LiuQH17}、序列标注\cite{DBLP:conf/acl/SogaardG16}等常见NLP任务上均取得了成功。

多任务学习的一个关键问题在于如何设计一个高效的共享模式来允许模型共享多个任务的知识。上述提到的工作也大都致力于为所要解决的问题以及采用的网络结构来设计合适的共享模式，如硬共享模式、软共享模式、分层共享模式、共享-私有模式等。

同时，也有大量工作致力于使用迁移学习的范式来获取文本的通用表示，一般做法是利用语言模型\cite{DBLP:conf/naacl/PetersNIGCLZ18}\cite{radford2018improving}、机器翻译\cite{DBLP:conf/nips/McCannBXS17}或其他无监督任务\cite{devlin2018bert}来预训练一个可迁移的模型。并且，迁移学习与多任务学习本身并不互斥，因此可以同时利用二者的方法，使用多任务预训练迁移模型\cite{DBLP:conf/iclr/SubramanianTBP18}，也可以在预训练得到的模型的基础上再使用多任务来微调\cite{liu2019multi}\cite{anonymous2018bam!}。

事实上，多任务学习和迁移学习本质上都是通过共享参数来迁移模型在不同任务中学习到的知识，并以此来提升泛化能力。因此，通常在迁移学习中效果很好的模型也可以应用在多任务学习中。近期，以Transformer为预训练网络结构得到的迁移模型GPT\cite{radford2018improving}和BERT\cite{devlin2018bert}在诸多自然语言处理任务上取得了极大的提升，这证明了Transformer强大的文本表示能力。然而，不同于CNN和RNN，目前还很少有工作研究多任务学习在Transformer上的应用模式，已有的少量工作也只是将最传统的多任务共享模式简单地应用在Transformer中\cite{liu2019multi}。

\section{本文研究内容}

本文试图在一定程度上填补目前多任务学习在Transformer结构下的研究空缺，探索基于Transformer的多任务共享模式，并通过实验比较几种多任务Transformer结构的效果。首先，本文考察了传统的硬共享模式在Transformer上的应用效果，然后，根据Transformer自身的结构特点设计了新的多任务架构。

在CNN及RNN结构中应用多任务学习通常是“纵向”的，如硬共享模式和分层共享模式，即在网络结构的某一层上堆叠任务特定层\cite{DBLP:conf/acl/SogaardG16}\cite{DBLP:conf/ijcai/ZhengCQ18}。这种架构蕴含着一个假设：存在某种通用的文本表示，特定的任务表示可以由该通用表示通过简单的变换得到。然而，这样的假设限制了能够同时学习的任务的多样性，难以处理弱相关任务及不同难度的任务。此外，任务特定层的位置通常需要根据任务特点来人为设定，这增大了多任务学习的应用难度。也有一些工作采用了“横向”共享的架构，如软共享模式和共享-私有模式，即允许模型使用其他任务的模型在同一层的隐状态\cite{DBLP:conf/ijcai/LiuQH16}\cite{DBLP:conf/cvpr/MisraSGH16}\cite{1705.08142}，然而这些架构也存在两个问题：（1）通常需要为每个任务训练一个模型，参数量大且难以扩展；（2）各任务模型之间的信息交互难以控制。

而在Transformer中，可以很容易地在“横向”以记号的形式进行扩展，并且扩展的记号可以像普通单词一样与句子中的每个单词进行交互。同时，相比软共享、共享-私有模式等“横向”共享方法，这种共享结构只需增加少量参数。基于这一特性，本文给出了两种新型多任务共享模式：层级-隐式共享模式（L-I结构）和层级-显式共享模式（L-E结构）。在十六个文本分类数据集上进行的实验表明，本文提出的层级-隐式共享结构只需要增加0.5\%的参数量就可以提升约5\%的平均分类准确率。

此外，建模多任务之间的联系一直是多任务学习领域的研究重点。层级-显式共享结构可以直接利用注意力机制对任意任务之间的关系进行建模。并且，由于注意力机制的引入，本文的多任务Transformer结构具备良好的可解释性。对注意力得分矩阵的可视化为模型的预测结果提供了证据，展示了任务之间的交互关系。

\section{论文结构}

本文主要内容包括介绍已有的基于多任务学习的文本表示方法，几种新型的多任务文本表示模型及其实验结果，工作总结及对未来的展望。全文分为五个章节进行介绍，具体结构安排如下：

第~\ref{cha:intro}~章，介绍研究背景及意义，概括本文的研究内容。

第~\ref{cha:related_work}~章，介绍相关的理论基础及前沿进展。在~\ref{sec:dl}~节介绍深度学习的相关概念；在~\ref{sec:mtl}~节介绍基于神经网络的多任务学习；在~\ref{sec:nlp}~节介绍深度学习背景下自然语言处理的发展现状；在~\ref{sec:mtl4nlp}~节介绍多任务学习在自然语言处理中的应用。

第~\ref{cha:model}~章，详细描述本文设计的模型结构及实现细节。在~\ref{sec:tf}~节介绍Transformer的模型结构；在~\ref{sec:mtl_tf}~节设计了几种多任务Transformer架构；在~\ref{sec:imp}~节给出本文所使用的超参数设置以及实现细节。

第~\ref{cha:exp}~章，介绍实验相关的信息。在~\ref{sec:task}~节描述模型应用的具体任务；在~\ref{sec:ds}~节给出本文使用的各个数据集的有关信息；在~\ref{sec:results}~节对比了各模型在各数据集上的实验结果；最后在~\ref{sec:analysis}~节对模型进行了实例可视化分析。

第~\ref{cha:conclusions}~章，对本文的贡献和不足进行总结，回顾相关领域面临的机遇与挑战，并给出了未来的研究方向。

